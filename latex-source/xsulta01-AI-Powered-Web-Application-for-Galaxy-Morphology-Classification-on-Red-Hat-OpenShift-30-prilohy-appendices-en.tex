% This file should be replaced with your file with an appendices (headings below are examples only)

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it and change
%\documentclass[../projekt.tex]{subfiles}
%\begin{document}

% Placing of table of contents of the memory media here should be consulted with a supervisor
%\chapter{Contents of the included storage media}

%\chapter{Manual}

%\chapter{Configuration file}

%\chapter{Scheme of RelaxNG configuration file}

%\chapter{Poster}

\chapter{Implementation Code Examples}
\label{ap:code-examples}

This chapter contains the code examples described in the \autoref{chapter:implementation}

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={Cosmoformer class code example}, label={lst:be-model}]
class Cosmoformer:
    def __init__(self, model_path: str = "model/cosmoformer_traced_cpu.pt"):
        """
        Initialize the Cosmoformer model.
        """
        self.model = torch.jit.load(model_path, map_location="cpu")
        self.model.eval()
        self.transform = v2.Compose([
            v2.Resize((224, 224)),
            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])  # v2.ToTensor()
        ])

    def predict(self, image: Image.Image) -> str:
        """
        Perform a forward pass on a Image.
        Return the predicted galaxy class label.
        """
        tensor = self.transform(image)  # shape: [3, 224, 224]
        tensor = tensor.unsqueeze(0)    # shape: [1, 3, 224, 224]
        with torch.no_grad():
            output = self.model(tensor)
            predicted_idx = torch.argmax(output, dim=1).item()
        return LABELS.get(predicted_idx, "unknown")
\end{lstlisting}
\end{samepage}

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={Dataset partitioning code example}, label={lst:data-split}]
df_train, df_val = train_test_split(
    train_catalog, test_size=0.25, random_state=42, shuffle=True
)
df_train.reset_index(drop=True, inplace=True)
df_val.reset_index(drop=True, inplace=True)
test_catalog.reset_index(drop=True, inplace=True)
\end{lstlisting}
\end{samepage}

\newpage

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={Validation loop code example}, label={lst:loop-val}]
# Validation phase

model.eval()
running_val_loss = 0.0
correct, total = 0, 0

with torch.no_grad():
    for images, labels in val_loader:
        images, labels = images.to(device), labels.to(device)
        
        # Mixed precision inference also possible (faster on GPUs)
        with autocast(device_type='cuda'):
            outputs = model(images)
            loss = criterion(outputs, labels)
        
        running_val_loss += loss.item()
        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()
        total   += labels.size(0)
        val_tqdm.set_postfix(loss=loss.item())

val_loss = running_val_loss / len(val_loader)
val_acc = correct / total
print(f"Epoch [{epoch+1}/{num_epochs}] Summary: Val Acc={val_acc:.4f}")

if (val_acc > best_val_acc):
    best_val_acc = val_acc
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
    }
    checkpoint_filename = f"checkpoints/{val_acc:.4f}_epoch_{epoch+1}.pth"
    torch.save(checkpoint, checkpoint_filename)
scheduler.step()
\end{lstlisting}
\end{samepage}

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={Dataset load code example}, label={lst:data-load}]
train_gz2 = GZ2(root='gz2', train=True, download=True)
test_gz2  = GZ2(root='gz2', train=False, download=True)

train_catalog = train_gz2[['filename', 'summary']]
test_catalog  = test_gz2[['filename', 'summary']]

# Replace empty summaries
train_catalog['summary'] = train_catalog['summary'].fillna('irregular')
test_catalog['summary']  = test_catalog['summary'].fillna('irregular')

# Rename "summary" into "label"
train_catalog.rename(columns={'summary': 'label'}, inplace=True)
test_catalog.rename(columns={'summary': 'label'}, inplace
\end{lstlisting}
\end{samepage}

\newpage

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={Dataset saving code example}, label={lst:data-save}]
def copy_and_update(df, target_dir):
    new_paths = []
    for i, row in df.iterrows():
        src_path = row['orig_img_path']
        filename = str(i) + '_' + target_dir + ".jpg"
        dst_path = os.path.join(target_dir, filename)
        try:
            shutil.copy2(src_path, dst_path)
        except Exception as e:
            print(f"Error copying {src_path} to {dst_path}: {e}")
        new_paths.append(dst_path)
    df['img_path'] = new_paths
    return df

df_train = copy_and_update(df_train, 'train')
df_val = copy_and_update(df_val, 'validation')
df_test = copy_and_update(test_catalog, 'test')

df_train[['img_path', 'label']].to_parquet('train.parquet', index=False)
df_val[['img_path', 'label']].to_parquet('validation.parquet', index=False)
df_test[['img_path', 'label']].to_parquet('test.parquet', index=False)
\end{lstlisting}
\end{samepage}

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={FastAPI endpoint decorator code example}, label={lst:be-post}]
@app.post("/inference", response_model=InferenceResponse)
async def inference(file: UploadFile):
    file_bytes = await file.read()
    image = Image.open(io.BytesIO(file_bytes)).convert("RGB")
    predicted_class = cosmoformer.predict(image)
    logger.debug(f"Predicted class: {predicted_class}")

    return InferenceResponse(predicted_class=predicted_class)
\end{lstlisting}
\end{samepage}

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={Training setup code example}, label={lst:loop-setup}]
num_epochs = 20  # total number of epochs
warmup_epochs = round(num_epochs * 0.1)
base_lr = 2e-6
weight_decay = 1e-4

def lr_lambda(epoch):
    if epoch < warmup_epochs:
        return float(epoch + 1) / warmup_epochs
    else:
        progress = (epoch - warmup_epochs) / (num_epochs - warmup_epochs)
        return 0.5 * (1.0 + math.cos(math.pi * progress))

scaler = GradScaler("cuda")
optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)
scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)
\end{lstlisting}
\end{samepage}

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={TorchScript model saving code example}, label={lst:model-save}]
model = model.to("cpu")

example_input = torch.randn(1, 3, 224, 224, device="cpu")
traced_model = torch.jit.trace(model, example_input)

torch.jit.save(traced_model, "cosmoformer_traced_cpu.pt")
\end{lstlisting}
\end{samepage}

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={Test phase code example}, label={lst:loop-test}]
model.eval()

test_loss = 0.0
correct = 0
total = 0

test_tqdm = tqdm(test_loader, desc="Testing", smoothing=0.9)

with torch.no_grad():
    for images, labels in test_tqdm:
        images, labels = images.to(device), labels.to(device)
        
        outputs = model(images)
        loss = criterion(outputs, labels)
        test_loss += loss.item()
        
        _, predicted = torch.max(outputs, dim=1)
        correct += (predicted == labels).sum().item()
        total += labels.size(0)
        
        test_tqdm.set_postfix(loss=loss.item())

test_loss /= len(test_loader)
test_acc = correct / total

print(f"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}")
\end{lstlisting}
\end{samepage}

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={FastAPI initialization code example}, label={lst:be-life}]
@asynccontextmanager
async def lifespan(app: FastAPI):
    try:
        logger.info('Loading the Cosmoformer model')
        global cosmoformer
        cosmoformer = Cosmoformer(model_path=settings.MODEL_PATH)
        yield
    except Exception as e:
        logger.error("Error has occurred while loading Cosmoformer model: ",e)
    finally:
        logger.info('Shutting down the app...')

app = FastAPI(lifespan=lifespan)
\end{lstlisting}
\end{samepage}

\newpage

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={DataLoader code example}, label={lst:data-DataLoader}]
batch_size = 64

train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)

val_loader = DataLoader(
    validation_dataset,
    batch_size=batch_size,
    shuffle=False
)

test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    shuffle=False
)
\end{lstlisting}
\end{samepage}

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={Dataset subclass code example}, label={lst:data-Dataset}]
class ParquetImageDataset(Dataset):
    """
    A PyTorch Dataset that reads a Parquet file containing:
      - img_path: path to image on disk
      - label: label of the image
    """
    def __init__(self, parquet_file, transform=None, label_encoder=None):
        super().__init__()
        self.data = pd.read_parquet(parquet_file)
        self.transform = transform
        self.label_encoder = label_encoder

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        img_path = "cosmoformer-dataset/"+row['img_path']
        label_str = row['label']
        image = Image.open(img_path).convert('RGB')

        if self.transform is not None:
            image = self.transform(image)
        if self.label_encoder is not None:
            label = self.label_encoder.transform([label_str])[0]
            label = torch.tensor(label, dtype=torch.long)
        else:
            label = label_str
        return image, label
\end{lstlisting}
\end{samepage}

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={React Dropzone code example}, label={lst:fe-dz}]
  <MyDropzone
    file={selectedFile}
    onFileAccepted={onFileAccepted}
    onError={onDropzoneError}
  />
  <div style={{ marginTop: '20px' }}>
    <button onClick={handleSubmit} disabled={!selectedFile || loading}>
      {loading ? 'Submitting...' : 'Submit'}
    </button>
    <button
      onClick={handleClear}
      disabled={!selectedFile && !error}
      style={{ marginLeft: '10px' }}
    >
      Clear
    </button>
  </div>
\end{lstlisting}
\end{samepage}

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={Training loop code example}, label={lst:loop-train}]
# Training phase

model.train()
running_train_loss = 0.0
    
for images, labels in train_loader:
    images, labels = images.to(device), labels.to(device)

    # Zero out the gradients
    optimizer.zero_grad()
    
    # 2) Mixed precision forward pass
    with autocast(device_type='cuda'):
        outputs = model(images)
        loss = criterion(outputs, labels)
    
    # 3) Backprop with scaled loss
    scaler.scale(loss).backward()
    
    # 4) Gradient clipping after unscaling
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    
    # 5) Step the optimizer with scaled gradients
    scaler.step(optimizer)
    scaler.update()
    
    running_train_loss += loss.item()
    train_tqdm.set_postfix(loss=loss.item())
    
train_loss = running_train_loss / len(train_loader)
\end{lstlisting}
\end{samepage}

\begin{samepage}
\begin{lstlisting}[style=mypython, caption={CrossFormer model initialization code example}, label={lst:model-model}]
model = CrossFormer(
    num_classes=len(le.classes_),       # number of output classes
    dim=(32, 64, 128, 256),             # dimension at each stage
    depth=(2, 2, 4, 2),                 # depth of transformer at each stage
    global_window_size=(8, 4, 2, 1),    # global window sizes at each stage
    local_window_size=7,                # local window size
    attn_dropout=0.1,
    ff_dropout=0.1
).to(device)
\end{lstlisting}
\end{samepage}

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it
%\end{document}
